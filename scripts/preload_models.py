# scripts/preload_models.py
import os
import sys
import shutil
import torch  # ç”¨äºæ¸…ç† GPU ç¼“å­˜
from transformers import AutoModel, AutoConfig, AutoProcessor, AutoTokenizer
import logging
import time

# --- é…ç½® ---
# é…ç½®æ—¥å¿—è®°å½•å™¨ï¼Œè¾“å‡ºåˆ°æ ‡å‡†è¾“å‡ºï¼Œæ–¹ä¾¿ Docker build æŸ¥çœ‹
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(name)s] - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("PreloadModels")

# è¦é¢„åŠ è½½çš„æ¨¡å‹åˆ—è¡¨:
#   name: Hugging Face Hub ä¸Šçš„æ¨¡å‹åç§°
#   has_tokenizer: æ˜¯å¦éœ€è¦é¢„åŠ è½½ Tokenizer
#   has_processor: æ˜¯å¦éœ€è¦é¢„åŠ è½½ Processor
MODELS_TO_PRELOAD = [
    {"name": "BAAI/bge-large-zh-v1.5", "has_tokenizer": True, "has_processor": False},
    {"name": "Marqo/marqo-fashionCLIP", "has_tokenizer": False, "has_processor": True}
]
MAX_ATTEMPTS = 3  # æ•´ä¸ªé¢„åŠ è½½è¿‡ç¨‹çš„æœ€å¤§é‡è¯•æ¬¡æ•°

# ä»ç¯å¢ƒå˜é‡è·å– HF_HOMEï¼Œè¿™å¿…é¡»ä¸ Dockerfile ä¸­çš„ ENV HF_HOME ä¸€è‡´ï¼
HF_HOME = os.environ.get("HF_HOME")
if not HF_HOME:
    logger.error("å…³é”®é”™è¯¯: HF_HOME ç¯å¢ƒå˜é‡æœªè®¾ç½®! æ— æ³•ç¡®å®šç¼“å­˜ç›®å½•ã€‚")
    logger.error("è¯·ç¡®ä¿ Dockerfile ä¸­è®¾ç½®äº† 'ENV HF_HOME=/runpod-volume' (æˆ–ç±»ä¼¼çš„æŒä¹…è·¯å¾„)")
    sys.exit(1) # å¿…é¡»å¤±è´¥é€€å‡º

# æ ¹æ® HF_HOME å®šä¹‰ç¼“å­˜è·¯å¾„
MODULES_CACHE = os.path.join(HF_HOME, "modules") # å­˜æ”¾ trust_remote_code=True ä¸‹è½½çš„ä»£ç 
HUB_CACHE = os.path.join(HF_HOME, "hub")       # å­˜æ”¾æ¨¡å‹æƒé‡ã€é…ç½®ç­‰ä¸»è¦æ–‡ä»¶
# --- é…ç½®ç»“æŸ ---

def clear_dynamic_modules_cache():
    """
    **æ ¸å¿ƒæ­¥éª¤**: æ¸…ç†ä¸º `trust_remote_code=True` åŠ¨æ€åŠ è½½çš„ä»£ç ç¼“å­˜ã€‚
    """
    logger.info("--- æ­£åœ¨æ¸…ç†åŠ¨æ€ä»£ç ç¼“å­˜ (`modules` ç›®å½•å’Œ `sys.modules`) ---")

    # 1. æ¸…ç†æ–‡ä»¶ç³»ç»Ÿç¼“å­˜ (`modules` ç›®å½•)
    if os.path.exists(MODULES_CACHE):
        logger.info(f"å°è¯•åˆ é™¤ Transformers æ¨¡å—ç¼“å­˜ç›®å½•: {MODULES_CACHE}")
        try:
            shutil.rmtree(MODULES_CACHE, ignore_errors=True)
            time.sleep(0.5) # çŸ­æš‚ç­‰å¾…
            if not os.path.exists(MODULES_CACHE):
                logger.info("æ¨¡å—ç¼“å­˜ç›®å½•å·²æˆåŠŸåˆ é™¤ã€‚")
            else:
                logger.warning("åˆ é™¤æ¨¡å—ç¼“å­˜ç›®å½•åå…¶ä»ç„¶å­˜åœ¨ (æƒé™é—®é¢˜æˆ–æ–‡ä»¶è¢«é”å®š?)")
        except Exception as e:
            logger.warning(f"åˆ é™¤æ¨¡å—ç¼“å­˜ç›®å½•æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    else:
        logger.info("æ¨¡å—ç¼“å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤ã€‚")

    # 2. æ¸…ç†å†…å­˜ä¸­å·²åŠ è½½çš„ Python æ¨¡å— (`sys.modules`)
    keys_to_delete = [k for k in sys.modules if k.startswith('transformers_modules')]
    if keys_to_delete:
        logger.info(f"æ­£åœ¨ä» sys.modules ä¸­ç§»é™¤ {len(keys_to_delete)} ä¸ªåŠ¨æ€åŠ è½½çš„ 'transformers_modules.*' æ¨¡å—...")
        for key in keys_to_delete:
            logger.debug(f"  åˆ é™¤ sys.modules['{key}']")
            try:
                del sys.modules[key]
            except KeyError:
                logger.debug(f"  é”® '{key}' åœ¨å°è¯•åˆ é™¤å‰å·²ä¸å­˜åœ¨äº sys.modules ä¸­ã€‚")
    else:
        logger.info("åœ¨ sys.modules ä¸­æœªæ‰¾åˆ°éœ€è¦ç§»é™¤çš„ 'transformers_modules.*' æ¨¡å—ã€‚")

    logger.info("--- åŠ¨æ€ä»£ç ç¼“å­˜æ¸…ç†å®Œæˆ ---")

def preload_single_model(model_info):
    """
    å°è¯•é¢„åŠ è½½å•ä¸ªæ¨¡å‹åŠå…¶ç›¸å…³ç»„ä»¶ã€‚è¿”å› True/Falseã€‚
    """
    model_name = model_info["name"]
    logger.info(f"--- === å¼€å§‹é¢„åŠ è½½æ¨¡å‹: {model_name} === ---")

    os.environ["HF_HUB_OFFLINE"] = "0"
    os.environ["TRANSFORMERS_OFFLINE"] = "0"

    # **å…³é”®**: åŠ è½½å‰æ¸…ç†åŠ¨æ€æ¨¡å—ç¼“å­˜
    clear_dynamic_modules_cache()

    config, model, tokenizer, processor = None, None, None, None
    success = False

    try:
        # 1. åŠ è½½é…ç½®
        logger.info(f"[{model_name}] æ­£åœ¨åŠ è½½é…ç½® (trust_remote_code=True)...")
        config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)
        logger.info(f"[{model_name}] é…ç½®åŠ è½½æˆåŠŸã€‚")

        # 2. åŠ è½½æ¨¡å‹
        logger.info(f"[{model_name}] æ­£åœ¨åŠ è½½æ¨¡å‹æƒé‡ (trust_remote_code=True)...")
        model = AutoModel.from_pretrained(model_name, config=config, trust_remote_code=True)
        logger.info(f"[{model_name}] æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸã€‚")

        # 3. åŠ è½½ Tokenizer
        if model_info.get("has_tokenizer"):
            logger.info(f"[{model_name}] æ­£åœ¨åŠ è½½ Tokenizer...")
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            logger.info(f"[{model_name}] Tokenizer åŠ è½½æˆåŠŸã€‚")

        # 4. åŠ è½½ Processor
        if model_info.get("has_processor"):
            logger.info(f"[{model_name}] æ­£åœ¨åŠ è½½ Processor...")
            try:
                processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
                logger.info(f"[{model_name}] Processor åŠ è½½æˆåŠŸ (ä½¿ç”¨äº† trust_remote_code)ã€‚")
            except TypeError:
                logger.warning(f"[{model_name}] Processor ä¸æ¥å— 'trust_remote_code' å‚æ•°ï¼Œå°è¯•ä¸å¸¦æ­¤å‚æ•°åŠ è½½...")
                try:
                    processor = AutoProcessor.from_pretrained(model_name)
                    logger.info(f"[{model_name}] Processor åŠ è½½æˆåŠŸ (æœªä½¿ç”¨ trust_remote_code)ã€‚")
                except Exception as e_proc_fallback:
                    logger.error(f"[{model_name}] åŠ è½½ Processor å¤±è´¥ (å³ä½¿æœªä½¿ç”¨ trust_remote_code): {e_proc_fallback}", exc_info=True)
            except Exception as e_proc:
                logger.error(f"[{model_name}] ä½¿ç”¨ trust_remote_code åŠ è½½ Processor å¤±è´¥: {e_proc}", exc_info=True)

        logger.info(f"--- === æ¨¡å‹ {model_name} é¢„åŠ è½½æˆåŠŸ === ---")
        success = True

    except Exception as e:
        logger.error(f"âŒâŒâŒ æ¨¡å‹ {model_name} é¢„åŠ è½½å¤±è´¥: {e} âŒâŒâŒ", exc_info=True)
        success = False

    finally:
        # æ¸…ç†å†…å­˜
        logger.debug(f"[{model_name}] æ¸…ç†å†…å­˜ä¸­çš„ Python å¯¹è±¡...")
        del config, model, tokenizer, processor
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.debug(f"[{model_name}] å·²æ¸…ç† CUDA ç¼“å­˜ã€‚")
        logger.debug(f"[{model_name}] å†…å­˜æ¸…ç†å®Œæˆã€‚")

    return success

# --- ä¸»æ‰§è¡Œé€»è¾‘ ---
logger.info(f"===== å¼€å§‹ Hugging Face æ¨¡å‹é¢„åŠ è½½æµç¨‹ =====")
logger.info(f"ä½¿ç”¨çš„ HF_HOME (ç¼“å­˜ç›®å½•): {HF_HOME}") # ä»ç„¶è®°å½• HF_HOMEï¼Œè¿™å¾ˆé‡è¦
logger.info(f"ç›®æ ‡æ¨¡å‹åˆ—è¡¨: {[m['name'] for m in MODELS_TO_PRELOAD]}")

overall_success = False
for attempt in range(MAX_ATTEMPTS):
    logger.info(f"ğŸš€ ç¬¬ {attempt + 1}/{MAX_ATTEMPTS} æ¬¡å°è¯•é¢„åŠ è½½æ‰€æœ‰æ¨¡å‹...")
    models_loaded_this_attempt = 0
    failed_models_this_attempt = []

    for model_info in MODELS_TO_PRELOAD:
        if preload_single_model(model_info):
            models_loaded_this_attempt += 1
        else:
            failed_models_this_attempt.append(model_info["name"])
            # å¯é€‰ï¼šå¦‚æœå¸Œæœ›ä¸€æ—¦å¤±è´¥å°±ç«‹å³é‡è¯•ï¼Œå¯ä»¥åœ¨è¿™é‡ŒåŠ  break
            # break

    if models_loaded_this_attempt == len(MODELS_TO_PRELOAD):
        logger.info(f"âœ…âœ…âœ… ç¬¬ {attempt + 1} æ¬¡å°è¯•æˆåŠŸ! æ‰€æœ‰æ¨¡å‹å‡å·²é¢„åŠ è½½ã€‚")
        overall_success = True
        break
    else:
        logger.warning(f"âš ï¸ ç¬¬ {attempt + 1} æ¬¡å°è¯•æœªå®Œå…¨æˆåŠŸã€‚å¤±è´¥çš„æ¨¡å‹: {failed_models_this_attempt}")
        if attempt < MAX_ATTEMPTS - 1:
            wait_time = 5 * (attempt + 1)
            logger.info(f"å°†åœ¨ {wait_time} ç§’åè¿›è¡Œä¸‹ä¸€æ¬¡å°è¯•...")
            time.sleep(wait_time)
        else:
            logger.error("âŒâŒâŒ å·²è¾¾åˆ°æœ€å¤§é¢„åŠ è½½å°è¯•æ¬¡æ•°ã€‚")

# --- æœ€ç»ˆç»“æœ ---
if overall_success:
    logger.info("ğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰æ¨¡å‹é¢„åŠ è½½æµç¨‹æˆåŠŸå®Œæˆã€‚ ğŸ‰ğŸ‰ğŸ‰")
    sys.exit(0)
else:
    logger.error("ğŸ”¥ğŸ”¥ğŸ”¥ æ¨¡å‹é¢„åŠ è½½æµç¨‹æœ€ç»ˆå¤±è´¥ã€‚è¯·ä»”ç»†æ£€æŸ¥ Docker æ„å»ºæ—¥å¿—ä¸­çš„é”™è¯¯ä¿¡æ¯ã€‚ ğŸ”¥ğŸ”¥ğŸ”¥")
    sys.exit(1)