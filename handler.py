import runpod
from transformers import AutoTokenizer, AutoModel, AutoProcessor
from rembg import remove, new_session
from PIL import Image
import torch
import base64
import io
import logging
import traceback
import time

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

# âœ… åŠ è½½æ¨¡å‹ï¼ˆå…¨éƒ¨ä»æœ¬åœ°è·¯å¾„ï¼Œå¼ºåˆ¶ä¸è”ç½‘ï¼‰
tokenizer = AutoTokenizer.from_pretrained(
    "/runpod-volume/hub/models--BAAI--bge-large-zh-v1.5",
    trust_remote_code=True,
    local_files_only=True
)

text_model = AutoModel.from_pretrained(
    "/runpod-volume/hub/models--BAAI--bge-large-zh-v1.5",
    trust_remote_code=True,
    local_files_only=True
).cuda().half().eval()

image_model = AutoModel.from_pretrained(
    "/runpod-volume/hub/models--Marqo--marqo-fashionCLIP",
    trust_remote_code=True,
    local_files_only=True
).cuda().half().eval()

image_processor = AutoProcessor.from_pretrained(
    "/runpod-volume/hub/models--Marqo--marqo-fashionCLIP",
    trust_remote_code=True,
    local_files_only=True
)

rembg_session = new_session("u2netp")

# æ‰“å°å½“å‰GPUä¿¡æ¯
logging.info(f"ğŸš€ å½“å‰ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")

# CUDA é¢„çƒ­ - text_model
with torch.no_grad():
    dummy_inputs = tokenizer(["warmup"], padding=True, return_tensors="pt", truncation=True)
    dummy_inputs = {k: v.cuda().half() for k, v in dummy_inputs.items()}
    _ = text_model(**dummy_inputs).last_hidden_state.mean(dim=1)
logging.info("âœ… æ–‡æœ¬æ¨¡å‹ warmup å®Œæˆ")

# CUDA é¢„çƒ­ - image_model
with torch.no_grad():
    dummy_image = Image.new('RGB', (224, 224), color=(255, 255, 255))  # åˆ›å»ºä¸€å¼ ç™½å›¾
    processed = image_processor(images=dummy_image, return_tensors="pt")
    processed = {k: v.cuda().half() for k, v in processed.items()}
    _ = image_model.get_image_features(**processed, normalize=True)
logging.info("âœ… å›¾ç‰‡æ¨¡å‹ warmup å®Œæˆ")

# âœ… æ ¸å¿ƒå¤„ç†å‡½æ•°
def handler(job):
    logging.info(f"ğŸ“¥ ä»»åŠ¡è¾“å…¥å†…å®¹:\n{job}\nğŸ“„ ç±»å‹: {type(job)}")
    try:
        model_type = job["input"].get("model", "text")
        inputs = job["input"].get("data")
        logging.info(f"ğŸ“‹ inputså†…å®¹æ˜¯: {inputs} (ç±»å‹: {type(inputs)}, é•¿åº¦: {len(inputs) if inputs else 0})")
        if isinstance(inputs, str):
            inputs = [inputs]

        if not inputs:
            logging.warning("âš ï¸ æ•°æ®ä¸ºç©º")
            return {
                "output": {
                    "error": "Empty input provided."
                }
            }

        results = []

        if model_type == "text":
            logging.info(f"ğŸ”  æ–‡æœ¬åµŒå…¥å¤„ç†ï¼Œæ•°é‡: {len(inputs)}")
            start_time = time.time()

            # Tokenizeré˜¶æ®µ
            encoded = tokenizer(inputs, padding=True, return_tensors="pt", truncation=True)
            logging.info(f"ğŸ§© Tokenizerè¾“å‡º keys: {list(encoded.keys())}")

            tokenizer_time = time.time()
            logging.info(f"â±ï¸ Tokenizerè€—æ—¶: {tokenizer_time - start_time:.3f}s")

            # ä¼ é€åˆ°cudaå¹¶è½¬ä¸ºåŠç²¾åº¦
            encoded = {k: v.cuda().half() for k, v in encoded.items()}
            to_cuda_time = time.time()
            logging.info(f"â±ï¸ To CUDAè€—æ—¶: {to_cuda_time - tokenizer_time:.3f}s")

            # æ¨ç†é˜¶æ®µ
            with torch.no_grad():
                output = text_model(**encoded).last_hidden_state.mean(dim=1).cpu().tolist()

            inference_time = time.time()
            logging.info(f"â±ï¸ æ¨ç†è€—æ—¶: {inference_time - to_cuda_time:.3f}s")

            for emb in output:
                results.append(emb)

            total_time = time.time()
            logging.info(f"âœ… æ€»å¤„ç†æ—¶é—´: {total_time - start_time:.3f}s")

        elif model_type == "image":
            logging.info(f"ğŸ–¼ï¸ å›¾åƒåµŒå…¥å¤„ç†ï¼Œæ•°é‡: {len(inputs)}")
            start_time = time.time()

            images = []
            for i, img_str in enumerate(inputs):
                img_start_time = time.time()
                if img_str.startswith("data:image/"):
                    img_str = img_str.split(",")[1]
                image = Image.open(io.BytesIO(base64.b64decode(img_str)))
                decode_time = time.time()
                logging.info(f"ğŸ–¼ï¸ è§£ç ç¬¬{i}å¼ å›¾ç‰‡è€—æ—¶: {decode_time - img_start_time:.3f}s")

                image = remove(image, session=rembg_session).convert("RGB")
                rembg_time = time.time()
                logging.info(f"ğŸ§¹ å»èƒŒæ™¯ç¬¬{i}å¼ å›¾ç‰‡è€—æ—¶: {rembg_time - decode_time:.3f}s")

                images.append(image)

            # æ‰¹é‡å¤„ç†
            processed = image_processor(images=images, return_tensors="pt")
            processor_time = time.time()
            logging.info(f"ğŸ›ï¸ å›¾ç‰‡æ‰¹å¤„ç†è€—æ—¶: {processor_time - rembg_time:.3f}s")

            processed = {k: v.cuda().half() for k, v in processed.items()}

            with torch.no_grad():
                vectors = image_model.get_image_features(**processed, normalize=True).cpu().tolist()

            inference_time = time.time()
            logging.info(f"â±ï¸ å›¾ç‰‡æ¨ç†è€—æ—¶: {inference_time - processor_time:.3f}s")

            for vector in vectors:
                results.append(vector)

            total_time = time.time()
            logging.info(f"âœ… æ€»å›¾ç‰‡å¤„ç†æ—¶é—´: {total_time - start_time:.3f}s")

        logging.info(f"âœ… è¿”å›æ•°æ®ç»“æ„: {results}")
        return {
            "output": {
                "embeddings": results
            }
        }

    except Exception as e:
        logging.error(f"âŒ å‡ºç°å¼‚å¸¸: {str(e)}")
        traceback.print_exc()
        return {
            "output": {
                "error": str(e),
                "trace": traceback.format_exc()
            }
        }

# âœ… å¯åŠ¨ Serverless Worker
logging.info("ğŸŸ¢ Worker å·²å¯åŠ¨ï¼Œç­‰å¾…ä»»åŠ¡ä¸­...")
runpod.serverless.start({"handler": handler})