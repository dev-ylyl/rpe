from fastapi import FastAPI
from pydantic import BaseModel
import torch
from transformers import AutoModel
from rembg import remove, new_session
from PIL import Image
import base64
import io
import numpy as np
from contextlib import asynccontextmanager
import logging

# é…ç½®æ—¥å¿—ç›´æ¥è¾“å‡ºåˆ°æ§åˆ¶å°
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)

app = FastAPI()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """å†·å¯åŠ¨ä¼˜åŒ–ï¼šé¢„åŠ è½½æ¨¡å‹å¹¶æ˜¾ç¤ºæ˜¾å­˜å ç”¨"""
    logging.info("ğŸš€ åˆå§‹åŒ–æ¨¡å‹ä¸­...")
    
    # æ–‡æœ¬æ¨¡å‹ï¼ˆBAAIï¼‰
    app.state.text_model = AutoModel.from_pretrained(
        "BAAI/bge-large-zh-v1.5",
        trust_remote_code=True
    ).cuda().eval()
    
    # å›¾åƒæ¨¡å‹ï¼ˆMarqoï¼‰
    app.state.image_model = AutoModel.from_pretrained(
        "Marqo/marqo-fashionCLIP",
        trust_remote_code=True
    ).cuda().eval()
    
    # Rembgä¼šè¯
    app.state.rembg_session = new_session("isnet-general-use")
    
    logging.info(f"âœ… åˆå§‹åŒ–å®Œæˆ | æ˜¾å­˜å ç”¨: {torch.cuda.memory_allocated()/1024**2:.2f}MB")
    yield
    
    # æ¸…ç†GPUå†…å­˜
    torch.cuda.empty_cache()

app = FastAPI(lifespan=lifespan)

class Request(BaseModel):
    input: str | List[str]  # æ”¯æŒOpenAIæ ¼å¼çš„å­—ç¬¦ä¸²æˆ–æ•°ç»„
    model: str = "text-embedding"  # å¯é€‰ "text-embedding" æˆ– "image-embedding"
    user: str = None  # å…¼å®¹OpenAIå­—æ®µ

def process_image(image_b64: str, session) -> Image.Image:
    """Base64å›¾åƒå¤„ç†æµæ°´çº¿"""
    try:
        # å…¼å®¹çº¯Base64å’ŒDataURLæ ¼å¼
        if image_b64.startswith('data:image/'):
            image_b64 = image_b64.split(",")[1]
        img = Image.open(io.BytesIO(base64.b64decode(image_b64)))
        return remove(img, session=session).convert("RGB")
    except Exception as e:
        logging.error(f"å›¾åƒå¤„ç†å¤±è´¥: {str(e)}")
        raise

@app.post("/v1/embeddings")
async def create_embedding(request: Request):
    """OpenAIå…¼å®¹çš„åµŒå…¥ç«¯ç‚¹"""
    try:
        inputs = [request.input] if isinstance(request.input, str) else request.input
        
        # æ–‡æœ¬å¤„ç†
        if request.model == "text-embedding":
            logging.info(f"ğŸ“ å¤„ç†æ–‡æœ¬è¾“å…¥ï¼ˆé•¿åº¦: {len(inputs[0])}ï¼‰")
            with torch.no_grad():
                tokenized = app.state.text_model.tokenize(inputs)
                embeddings = app.state.text_model(**tokenized).last_hidden_state.mean(dim=1).tolist()
        
        # å›¾åƒå¤„ç†
        elif request.model == "image-embedding":
            logging.info(f"ğŸ–¼ï¸ å¤„ç†å›¾åƒè¾“å…¥ï¼ˆæ•°é‡: {len(inputs)}ï¼‰")
            embeddings = []
            for img_str in inputs:
                img = process_image(img_str, app.state.rembg_session)
                with torch.no_grad():
                    # æç®€å½’ä¸€åŒ– (æ›¿ä»£processor)
                    img_tensor = torch.tensor(np.array(img)).permute(2,0,1).unsqueeze(0).float().cuda()
                    img_tensor = (img_tensor - 127.5) / 127.5
                    embedding = app.state.image_model(img_tensor)[0].tolist()
                    embeddings.append(embedding)
        
        # æ„å»ºOpenAIæ ¼å¼å“åº”
        return {
            "object": "list",
            "data": [{
                "object": "embedding",
                "index": i,
                "embedding": emb
            } for i, emb in enumerate(embeddings)],
            "model": request.model,
            "usage": {
                "prompt_tokens": len(inputs),
                "total_tokens": len(inputs)
            }
        }
    except Exception as e:
        logging.error(f"âŒ è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}")
        return {
            "error": {
                "message": str(e),
                "type": "invalid_request_error"
            }
        }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "gpu_available": torch.cuda.is_available(),
        "memory_used": f"{torch.cuda.memory_allocated()/1024**2:.2f}MB"
    }